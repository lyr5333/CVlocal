{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ccecdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import os\n",
    "import enum\n",
    "\n",
    "# Visualization related imports\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "# Main computation libraries\n",
    "import numpy as np\n",
    "\n",
    "# Deep learning related imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "class GATLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation #3 was inspired by PyTorch Geometric: https://github.com/rusty1s/pytorch_geometric\n",
    "\n",
    "    But, it's hopefully much more readable! (and of similar performance)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # We'll use these constants in many functions so just extracting them here as member fields\n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    # These may change in the inductive setting - leaving it like this for now (not future proof)\n",
    "    nodes_dim = 0      # node dimension (axis is maybe a more familiar term nodes_dim is the position of \"N\" in tensor)\n",
    "    head_dim = 1       # attention head dim\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        self.concat = concat  # whether we should concatenate or average the attention heads\n",
    "        self.add_skip_connection = add_skip_connection\n",
    "\n",
    "        #\n",
    "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
    "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
    "        #\n",
    "\n",
    "        # You can treat this one matrix as num_of_heads independent W matrices\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # After we concatenate target node (node i) and source node (node j) we apply the \"additive\" scoring function\n",
    "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
    "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        # Bias is definitely not crucial to GAT - feel free to experiment (I pinged the main author, Petar, on this one)\n",
    "        if bias and concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n",
    "        elif bias and not concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        if add_skip_connection:\n",
    "            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "        else:\n",
    "            self.register_parameter('skip_proj', None)\n",
    "\n",
    "        #\n",
    "        # End of trainable weights\n",
    "        #\n",
    "\n",
    "        self.leakyReLU = nn.LeakyReLU(0.2)  # using 0.2 as in the paper, no need to expose every setting\n",
    "        self.activation = activation\n",
    "        # Probably not the nicest design but I use the same module in 3 locations, before/after features projection\n",
    "        # and for attention coefficients. Functionality-wise it's the same as using independent modules.\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.log_attention_weights = log_attention_weights  # whether we should log the attention weights\n",
    "        self.attention_weights = None  # for later visualization purposes, I cache the weights here\n",
    "\n",
    "        self.init_params()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "        in_nodes_features, edge_index = data  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n",
    "\n",
    "        # shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node\n",
    "        # We apply the dropout to all of the input node features (as mentioned in the paper)\n",
    "        in_nodes_features = self.dropout(in_nodes_features)\n",
    "\n",
    "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp they did dropout here as well\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n",
    "\n",
    "        # shape = (E, NH, 1)\n",
    "        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
    "        # Add stochasticity to neighborhood aggregation\n",
    "        attentions_per_edge = self.dropout(attentions_per_edge)\n",
    "\n",
    "        #\n",
    "        # Step 3: Neighborhood aggregation\n",
    "        #\n",
    "\n",
    "        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul\n",
    "        # shape = (E, NH, FOUT) * (E, NH, 1) -> (E, NH, FOUT), 1 gets broadcast into FOUT\n",
    "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
    "\n",
    "        # This part sums up weighted and projected neighborhood feature vectors for every target node\n",
    "        # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
    "\n",
    "        #\n",
    "        # Step 4: Residual/skip connections, concat and bias\n",
    "        #\n",
    "\n",
    "        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
    "        return (out_nodes_features, edge_index)\n",
    "\n",
    "    #\n",
    "    # Helper functions (without comments there is very little code so don't be scared!)\n",
    "    #\n",
    "\n",
    "    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n",
    "        \"\"\"\n",
    "        As the fn name suggest it does softmax over the neighborhoods. Example: say we have 5 nodes in a graph.\n",
    "        Two of them 1, 2 are connected to node 3. If we want to calculate the representation for node 3 we should take\n",
    "        into account feature vectors of 1, 2 and 3 itself. Since we have scores for edges 1-3, 2-3 and 3-3\n",
    "        in scores_per_edge variable, this function will calculate attention scores like this: 1-3/(1-3+2-3+3-3)\n",
    "        (where 1-3 is overloaded notation it represents the edge 1-3 and its (exp) score) and similarly for 2-3 and 3-3\n",
    "         i.e. for this neighborhood we don't care about other edge scores that include nodes 4 and 5.\n",
    "\n",
    "        Note:\n",
    "        Subtracting the max value from logits doesn't change the end result but it improves the numerical stability\n",
    "        and it's a fairly common \"trick\" used in pretty much every deep learning framework.\n",
    "        Check out this link for more details:\n",
    "\n",
    "        https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning\n",
    "\n",
    "        \"\"\"\n",
    "        # Calculate the numerator. Make logits <= 0 so that e^logit <= 1 (this will improve the numerical stability)\n",
    "        scores_per_edge = scores_per_edge - scores_per_edge.max()\n",
    "        exp_scores_per_edge = scores_per_edge.exp()  # softmax\n",
    "\n",
    "        # Calculate the denominator. shape = (E, NH)\n",
    "        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\n",
    "\n",
    "        # 1e-16 is theoretically not needed but is only there for numerical stability (avoid div by 0) - due to the\n",
    "        # possibility of the computer rounding a very small number all the way to 0.\n",
    "        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\n",
    "\n",
    "        # shape = (E, NH) -> (E, NH, 1) so that we can do element-wise multiplication with projected node features\n",
    "        return attentions_per_edge.unsqueeze(-1)\n",
    "\n",
    "    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\n",
    "        # The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -> (E, NH)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge).long()\n",
    "\n",
    "        # shape = (N, NH), where N is the number of nodes and NH the number of attention heads\n",
    "        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes\n",
    "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n",
    "\n",
    "        # position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the\n",
    "        # target index)\n",
    "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n",
    "\n",
    "        # Expand again so that we can use it as a softmax denominator. e.g. node i's sum will be copied to\n",
    "        # all the locations where the source nodes pointed to i (as dictated by the target index)\n",
    "        # shape = (N, NH) -> (E, NH)\n",
    "        return neighborhood_sums.index_select(self.nodes_dim, trg_index.long())\n",
    "\n",
    "    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n",
    "        size = list(nodes_features_proj_lifted_weighted.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes  # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n",
    "\n",
    "        # shape = (E) -> (E, NH, FOUT)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted).long()\n",
    "        # aggregation step - we accumulate projected, weighted node features for all the attention heads\n",
    "        # shape = (E, NH, FOUT) -> (N, NH, FOUT)\n",
    "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n",
    "\n",
    "        return out_nodes_features\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "\n",
    "        \"\"\"\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim].long()\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim].long()\n",
    "\n",
    "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "    def explicit_broadcast(self, this, other):\n",
    "        # Append singleton dimensions until this.dim() == other.dim()\n",
    "        for _ in range(this.dim(), other.dim()):\n",
    "            this = this.unsqueeze(-1)\n",
    "\n",
    "        # Explicitly expand so that shapes are the same\n",
    "        return this.expand_as(other)\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
    "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
    "\n",
    "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
    "        Feel free to experiment - there may be better initializations depending on your problem.\n",
    "\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\n",
    "        if self.log_attention_weights:  # potentially log for later visualization in playground.py\n",
    "            self.attention_weights = attention_coefficients\n",
    "\n",
    "        if self.add_skip_connection:  # add skip or residual connection\n",
    "            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if FIN == FOUT\n",
    "                # unsqueeze does this: (N, FIN) -> (N, 1, FIN), out features are (N, NH, FOUT) so 1 gets broadcast to NH\n",
    "                # thus we're basically copying input vectors NH times and adding to processed vectors\n",
    "                out_nodes_features += in_nodes_features.unsqueeze(1)\n",
    "            else:\n",
    "                # FIN != FOUT so we need to project input feature vectors into dimension that can be added to output\n",
    "                # feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.\n",
    "                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "        if self.concat:\n",
    "            # shape = (N, NH, FOUT) -> (N, NH*FOUT)\n",
    "            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\n",
    "        else:\n",
    "            # shape = (N, NH, FOUT) -> (N, FOUT)\n",
    "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out_nodes_features += self.bias\n",
    "\n",
    "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self,latent_size,node_num,num_features,num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,\n",
    "                 dropout=0.6, log_attention_weights=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        num_heads_per_layer = [1] + num_heads_per_layer\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            latent_size,node_num,num_features,num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection, bias,\n",
    "                 dropout, log_attention_weights)\n",
    "        self.decoder = Decoder(\n",
    "            latent_size,node_num,num_features,num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection, bias,\n",
    "                 dropout, log_attention_weights)\n",
    "\n",
    "    def forward(self, x,edge_index):\n",
    "        x = (x,edge_index)\n",
    "        means, log_var = self.encoder(x)\n",
    "        z = self.sample_from_gaussian(means, log_var)\n",
    "        means_x, log_var_x = self.decoder(z,edge_index)\n",
    "        x = self.sample_from_gaussian(means_x, log_var_x)\n",
    "        return means_x,log_var_x, means, log_var,z,x\n",
    "\n",
    "\n",
    "    \n",
    "    def reconstruction_probability(self,x,edge_index):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        enc = (x,edge_index)\n",
    "        mu, logvar = self.encoder(enc)\n",
    "        x = x.view(-1)\n",
    "        z_samples = self.samples_from_gaussian(mu,logvar)\n",
    "#         likelyhoods = torch.tensor(float(0))\n",
    "        inum = 0\n",
    "        loss = 0\n",
    "        old_zb = 0\n",
    "        for ind , z_sample in enumerate(z_samples):\n",
    "            x_mean, x_logvar = self.decoder(z_sample,edge_index)\n",
    "            log_recon_likelihood = -0.5 * (torch.sum(torch.pow(x-x_mean,2) * torch.exp(-x_logvar) , axis = [0]) + torch.sum(x_logvar ,axis=0) + args.node_num * np.log(2*np.pi))\n",
    "            T = log_recon_likelihood\n",
    "            kl_divergence = -0.5 * torch.sum(1 + logvar - torch.pow(mu,2) - torch.exp(logvar), axis=0)\n",
    "            if ind == 0:\n",
    "                loss = torch.mean(kl_divergence - log_recon_likelihood)\n",
    "                old_zb = T\n",
    "            else:\n",
    "                loss +=torch.mean(kl_divergence - log_recon_likelihood)\n",
    "                old_zb += T\n",
    "\n",
    "        \n",
    "        return loss/10,old_zb/10\n",
    "    def reparameterize(self, mu, log_var):\n",
    "\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        return mu + eps * std\n",
    "                                     \n",
    "    def sample_from_gaussian(self, mean,logvar):\n",
    "        eps = torch.randn_like(mean)\n",
    "        return eps.mul(torch.exp(logvar/2)).add_(mean)\n",
    "    \n",
    "    def samples_from_gaussian(self, mean, logvar, T=10):\n",
    "        z_samples = []\n",
    "        for _ in range(T):\n",
    "            eps = torch.randn_like(mean)\n",
    "            z_samples.append(eps.mul(torch.exp(logvar/2)).add_(mean))\n",
    "        return z_samples\n",
    "    def inference(self, z):\n",
    "\n",
    "        recon_x = self.decoder(z)\n",
    "\n",
    "        return recon_x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,latent_size,node_num,num_features,num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection, bias,\n",
    "                 dropout, log_attention_weights):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        gat_layers = []  # collect GAT layers\n",
    "        for i in range(num_of_layers):\n",
    "            layer = GATLayer(\n",
    "                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation\n",
    "                num_out_features=num_features_per_layer[i+1],\n",
    "                num_of_heads=num_heads_per_layer[i+1],\n",
    "                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\n",
    "                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\n",
    "                dropout_prob=dropout,\n",
    "                add_skip_connection=add_skip_connection,\n",
    "                bias=bias,\n",
    "                log_attention_weights=log_attention_weights\n",
    "            )\n",
    "            gat_layers.append(layer)\n",
    "\n",
    "        self.MLP = nn.Sequential(\n",
    "            *gat_layers,\n",
    "        )\n",
    "        self.linear_means = nn.Linear(num_features*node_num, latent_size)\n",
    "        self.linear_log_var = nn.Linear(num_features*node_num, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.MLP(x)[0]\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = torch.squeeze(x)\n",
    "        means = self.linear_means(x)\n",
    "        log_vars = self.linear_log_var(x)\n",
    "\n",
    "        return means, log_vars\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,latent_size,node_num,num_features,num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection, bias,\n",
    "                 dropout, log_attention_weights):\n",
    "\n",
    "        super().__init__()\n",
    "        self.node_num = node_num\n",
    "        self.num_features = num_features\n",
    "        self.decoder_input = nn.Linear(latent_size,node_num*num_features)\n",
    "        gat_layers = []  # collect GAT layers\n",
    "        for i in range(num_of_layers):\n",
    "            layer = GATLayer(\n",
    "                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation\n",
    "                num_out_features=num_features_per_layer[i+1],\n",
    "                num_of_heads=num_heads_per_layer[i+1],\n",
    "                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\n",
    "                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\n",
    "                dropout_prob=dropout,\n",
    "                add_skip_connection=add_skip_connection,\n",
    "                bias=bias,\n",
    "                log_attention_weights=log_attention_weights\n",
    "            )\n",
    "            gat_layers.append(layer)\n",
    "\n",
    "        self.MLP = nn.Sequential(\n",
    "            *gat_layers,\n",
    "        )\n",
    "        self.linear_means = nn.Linear(num_features*node_num, num_features*node_num)\n",
    "        self.linear_log_var = nn.Linear(num_features*node_num, num_features*node_num)\n",
    "    def forward(self, z,edge_index):\n",
    "        z = self.decoder_input(z)\n",
    "        z = z.view(-1,self.node_num,self.num_features)\n",
    "        z = torch.squeeze(z, dim=0)\n",
    "        z = (z,edge_index)\n",
    "        x = self.MLP(z)[0]\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = torch.squeeze(x)\n",
    "        means = self.linear_means(x)\n",
    "        log_vars = self.linear_log_var(x)\n",
    "        return means,log_vars\n",
    "\n",
    "import torch.utils.data as data\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        img, target = self.images[index], self.labels[index]\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "def load_graph_data(edgepath,featpath):\n",
    "    edge_index_list = []\n",
    "    node_features_list = []\n",
    "    file=open(edgepath,\"rb\")\n",
    "    edge_list = pickle.load(file)\n",
    "    \n",
    "    dfnp = np.array(edge_list).reshape((-1,args.node_num,args.node_num))\n",
    "    dfnp = dfnp + np.identity(args.node_num)\n",
    "    for i in range(len(dfnp)):\n",
    "        size = np.sum(dfnp[i]==1)\n",
    "        midnp = np.zeros((2,size))\n",
    "        sign = 0\n",
    "        for j in range(args.node_num):\n",
    "            for t in range(args.node_num):\n",
    "                if dfnp[i][j][t] ==1:\n",
    "                    midnp[0][sign] =j\n",
    "                    midnp[1][sign] =t\n",
    "                    sign+=1\n",
    "        edge_index_list.append(midnp)\n",
    "\n",
    "    fun = np.loadtxt(open(featpath,\"rb\"), delimiter=\",\", skiprows=0)\n",
    "    funnp = fun.reshape((-1,args.node_num,1))\n",
    "    \n",
    "    \n",
    "    def get_mean_std(matrix):\n",
    "        mean = []\n",
    "        std = []\n",
    "        for item in np.transpose(matrix):\n",
    "            mean.append(np.mean(item[item>0.00001]))\n",
    "            std.append(max(1, np.std(item[item>0.00001])))\n",
    "        return mean, std\n",
    "    def normalization(matrix, mean, std):\n",
    "        n_mat = np.array(matrix, dtype=np.float32)\n",
    "        n_mat = np.where(n_mat<0.00001, 0, (n_mat - mean) / std)\n",
    "        return n_mat\n",
    "\n",
    "    \n",
    "    funnp = funnp.reshape((-1,args.node_num))\n",
    "    mean,std = get_mean_std(funnp)\n",
    "    funnp = normalization(funnp, mean, std)\n",
    "    funnp = funnp.reshape((-1,args.node_num,1))\n",
    "    return (funnp, edge_index_list),mean,std\n",
    "\n",
    "\n",
    "def load_val_data(edgepath,featpath,mean,std):\n",
    "    valfeatureslist = np.loadtxt(open(featpath,\"rb\"), delimiter=\",\", skiprows=0)\n",
    "    file=open(edgepath,\"rb\")\n",
    "    valedge_list = pickle.load(file)\n",
    "    \n",
    "    valedge_list = np.array(valedge_list).reshape((-1,args.node_num,args.node_num))\n",
    "    valfeatureslist = valfeatureslist.reshape((-1,args.node_num,1))\n",
    "\n",
    "    valedge_list = valedge_list + np.identity(args.node_num)\n",
    "\n",
    "    val_edge_index_list = []\n",
    "    for i in range(len(valedge_list)):\n",
    "        size = np.sum(valedge_list[i]==1)\n",
    "        midnp = np.zeros((2,size))\n",
    "        sign = 0\n",
    "        for j in range(args.node_num):\n",
    "            for t in range(args.node_num):\n",
    "                if valedge_list[i][j][t] ==1:\n",
    "                    midnp[0][sign] =j\n",
    "                    midnp[1][sign] =t\n",
    "                    sign+=1\n",
    "        val_edge_index_list.append(midnp)\n",
    "    \n",
    "        \n",
    "\n",
    "    def normalization(matrix, mean, std):\n",
    "        n_mat = np.array(matrix, dtype=np.float32)\n",
    "        n_mat = np.where(n_mat<0.00001, 0, (n_mat - mean) / std)\n",
    "        return n_mat\n",
    "    \n",
    "    \n",
    "    valfeatureslist = valfeatureslist.reshape((-1,args.node_num))\n",
    "    valfeatureslist = normalization(valfeatureslist, mean, std)\n",
    "    valfeatureslist = valfeatureslist.reshape((-1,args.node_num,1))\n",
    "    \n",
    "    return (valfeatureslist, val_edge_index_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def main(args):\n",
    "    trainfeatpath = ''\n",
    "    trainedgepath = ''\n",
    "    ooo = 0\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #显存较大的显卡可以使用batch拼接实现批次计算，显存不足可以采用单图计算\n",
    "    data_loader,m,n = load_graph_data(trainedgepath,trainfeatpath)\n",
    "    dataset = MyDataset(data_loader[0], data_loader[1])\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    #valfeatpath = ''\n",
    "    #valedgepath = ''\n",
    "    # val_data_loader = load_val_data(valedgepath,valfeatpath,m,n)\n",
    "    # datasetval = MyDataset(val_data_loader[0], val_data_loader[1])\n",
    "    # data_loaderval = DataLoader(\n",
    "    #     dataset=datasetval, batch_size=1, shuffle=True)\n",
    "    \n",
    "\n",
    "    \n",
    "            \n",
    "    vae = VAE(\n",
    "        node_num=args.node_num,\n",
    "        latent_size=args.latent_size,\n",
    "        num_features=args.num_features,\n",
    "        num_of_layers=args.num_of_layers,\n",
    "        num_heads_per_layer=args.num_heads_per_layer,\n",
    "        num_features_per_layer=args.num_features_per_layer,\n",
    "        dropout=args.dropout).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=args.learning_rate)\n",
    "#     early_stopping = EarlyStopping(patience=10, verbose=False, path=f'/home/parameter/{args.experiment}.pth')\n",
    "    \n",
    "    logs = defaultdict(list)\n",
    "    firloss = 9999\n",
    "    inum = 0\n",
    "    lastloss = 9999\n",
    "    for epoch in range(args.epochs):\n",
    "        indexnum = 1\n",
    "        loss = 0\n",
    "        epochloss = 0\n",
    "        lossmm = 0\n",
    "        \n",
    "        for iteration, (node_features, edge_index) in enumerate(data_loader):\n",
    "            node_features = torch.squeeze(node_features, dim=0)\n",
    "            edge_index = torch.squeeze(edge_index)\n",
    "            node_features = torch.tensor(node_features, dtype=torch.float32)\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "            edge_index = edge_index.to(device)\n",
    "            node_features = node_features.to(device)\n",
    "            \n",
    "            mean_x,log_var_x ,mean, log_var,z,recon_x = vae(node_features,edge_index)\n",
    "\n",
    "            kl_divergence = -0.5 * torch.sum(1 + log_var - torch.pow(mean,2) - torch.exp(log_var), axis=0)\n",
    "            log_recon_likelihood = -0.5 * (torch.sum(torch.pow(node_features.reshape((args.node_num,))-mean_x.reshape((args.node_num,)),2) * torch.exp(-log_var_x.reshape((args.node_num,))) , axis = [0]) + torch.sum(log_var_x.reshape((args.node_num,)) ,axis=0) + args.node_num * np.log(2*np.pi))\n",
    "\n",
    "            \n",
    "            likeout = torch.mean(log_recon_likelihood)\n",
    "            nloss = torch.mean(kl_divergence - log_recon_likelihood)\n",
    "\n",
    "\n",
    "            lossmm+=nloss\n",
    "            if indexnum ==1:\n",
    "                loss = nloss\n",
    "                epochloss += nloss\n",
    "            else:\n",
    "                loss += nloss\n",
    "                epochloss += nloss\n",
    "            if indexnum % args.batch_size ==0:\n",
    "                loss = loss/args.batch_size \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss = 0\n",
    "            if indexnum == args.train_lenght:\n",
    "                loss = loss/(args.train_lenght - int(args.train_lenght/args.batch_size)*args.batch_size)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss = 0\n",
    "            indexnum+=1\n",
    "            if indexnum % 1000 ==0:\n",
    "                #print(f'阶段性loss:{lossmm/1000}')\n",
    "                lossmm =0\n",
    "        print(f'\\033[1;35;46m 第{epoch+1}轮 loss:   {epochloss/args.train_lenght} \\033[0m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc2a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", type=int, default=0)\n",
    "parser.add_argument(\"--epochs\", type=int, default=300)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256)\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001)\n",
    "parser.add_argument(\"--node_num\", type=int, default=100)\n",
    "parser.add_argument(\"--num_features\", type=list, default=1)\n",
    "parser.add_argument(\"--latent_size\", type=int, default=10)\n",
    "parser.add_argument(\"--print_every\", type=int, default=100)\n",
    "parser.add_argument(\"--fig_root\", type=str, default='figs')\n",
    "parser.add_argument(\"--num_of_layers\", type=int, default=3)\n",
    "parser.add_argument(\"--num_heads_per_layer\", type=list, default=[4, 6 , 4])\n",
    "parser.add_argument(\"--num_features_per_layer\", type=list, default=[1,6,4,1])\n",
    "parser.add_argument(\"--dropout\", type=float, default=0)\n",
    "parser.add_argument(\"--trace_type\", type=int, default=40)\n",
    "parser.add_argument(\"--train_lenght\", type=int, default=10000)\n",
    "parser.add_argument('--experiment', type=str, default='Abnormal_class_GAT_vae', help='experiment name')\n",
    "args = parser.parse_known_args()[0]\n",
    "#main(args)\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vae = VAE(\n",
    "        node_num=args.node_num,\n",
    "        latent_size=args.latent_size,\n",
    "        num_features=args.num_features,\n",
    "        num_of_layers=args.num_of_layers,\n",
    "        num_heads_per_layer=args.num_heads_per_layer,\n",
    "        num_features_per_layer=args.num_features_per_layer,\n",
    "        dropout=0).to(device)\n",
    "#加载训练好的模型\n",
    "parameter = torch.load(os.path.join(''))\n",
    "vae.load_state_dict(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73669093",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X1为数据集特征矩阵地址 E1为邻接矩阵地址，T1为调用链类型存储地址，L1为调用信息\n",
    "#nodelistxx为数据集中所有故障文件名称列表\n",
    "X1 = \"\"\n",
    "E1 = \"\"\n",
    "T1 = \"\"\n",
    "L1 = \"\"\n",
    "nodelistxx = [] \n",
    "def load_test_data1(mean,std):\n",
    "    nodenumxx = [0]\n",
    "    feattest = np.zeros((1,args.node_num,1))\n",
    "    edgetest = []\n",
    "    typely = np.zeros(1)\n",
    "    nodelistname = []\n",
    "    edgell = []\n",
    "    feattt = np.zeros((1,args.node_num,1))\n",
    "    for nodenamexx in nodelistxx : \n",
    "        X1x = X1.split(\"|\")[0] + nodenamexx +X1.split(\"|\")[1]\n",
    "        E1e = E1.split(\"|\")[0] + nodenamexx +E1.split(\"|\")[1]\n",
    "        T1t = T1.split(\"|\")[0] + nodenamexx +T1.split(\"|\")[1]\n",
    "        L1l = L1.split(\"|\")[0] + nodenamexx +L1.split(\"|\")[1]\n",
    "        \n",
    "        abnormal = np.loadtxt(open(X1x,\"rb\"), delimiter=\",\", skiprows=0)\n",
    "        file=open(E1e,\"rb\")\n",
    "        abnormalexg = pickle.load(file)\n",
    "        ynum2 = np.loadtxt(open(T1t,\"rb\"), delimiter=\",\", skiprows=0)\n",
    "        file=open(L1l,\"rb\")\n",
    "        yy2 = pickle.load(file)\n",
    "        edgell.extend(abnormalexg)\n",
    "        abnormalexg = np.array(abnormalexg).reshape((-1,args.node_num,args.node_num))\n",
    "        abnormal = abnormal.reshape((-1,args.node_num,1))\n",
    "        feattt = np.concatenate((feattt, abnormal), axis=0)\n",
    "        abnormalexg = abnormalexg + np.identity(args.node_num)\n",
    "        test_edge_index_list = []\n",
    "        for i in range(len(abnormalexg)):\n",
    "            size = np.sum(abnormalexg[i]==1)\n",
    "            midnp = np.zeros((2,size))\n",
    "            sign = 0\n",
    "            for j in range(args.node_num):\n",
    "                for t in range(args.node_num):\n",
    "                    if abnormalexg[i][j][t] ==1:\n",
    "                        midnp[0][sign] =j\n",
    "                        midnp[1][sign] =t\n",
    "                        sign+=1\n",
    "            test_edge_index_list.append(midnp)\n",
    "        \n",
    "        nodenumxx.append(len(abnormal)+nodenumxx[len(nodenumxx)-1])\n",
    "        feattest = np.concatenate((feattest, abnormal), axis=0)\n",
    "        typely = np.concatenate((typely, ynum2), axis=0)\n",
    "        nodelistname.extend(yy2)\n",
    "        edgetest.extend(test_edge_index_list)\n",
    "        \n",
    "    def normalization(matrix, mean, std):\n",
    "        n_mat = np.array(matrix, dtype=np.float32)\n",
    "        n_mat = np.where(n_mat<0.00001, 0, (n_mat - mean) / std)\n",
    "        return n_mat\n",
    "    \n",
    "    feattest = np.delete(feattest, 0, axis=0)\n",
    "    typely = np.delete(typely, 0, axis=0)\n",
    "    feattt = np.delete(feattt, 0, axis=0)\n",
    "    graphfunp = feattest.reshape((-1,args.node_num))\n",
    "    graphfunp = normalization(graphfunp, mean, std)\n",
    "    graphfunp = graphfunp.reshape((-1,args.node_num,1))\n",
    "    \n",
    "    return (graphfunp, edgetest,nodelistname,typely,nodenumxx,edgell,feattt)\n",
    "class MyDataset1(data.Dataset):\n",
    "    def __init__(self, images, labels,y,num_y):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.y = y\n",
    "        self.num_y = num_y\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        img, target ,y,num_y= self.images[index], self.labels[index],self.y[index],self.num_y[index]\n",
    "        return img, target,y,num_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_loader,m,n = load_graph_data()\n",
    "\n",
    "test_loader = load_test_data1(m,n)\n",
    "\n",
    "nodenumxx = test_loader[4]\n",
    "start_time = time.time() \n",
    "testset = MyDataset1(test_loader[0], test_loader[1],test_loader[2],test_loader[3])\n",
    "test_loader1 = DataLoader(dataset=testset, batch_size=1, shuffle=False)\n",
    "\n",
    "ablikely = []\n",
    "yy = []\n",
    "num_ylist = []\n",
    "itrll = 0\n",
    "for iteration, (node_features, edge_index,y,num_y) in enumerate(test_loader1):\n",
    "    node_features = torch.squeeze(node_features, dim=0)\n",
    "    edge_index = torch.squeeze(edge_index)\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float32)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "    edge_index = edge_index.to(device)\n",
    "    node_features = node_features.to(device)\n",
    "    loss,old_zb= vae.reconstruction_probability(node_features,edge_index)\n",
    "    yy.append(y)\n",
    "    ablikely.append(old_zb)\n",
    "    num_ylist.append(num_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655e50c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#加载分类阈值和正常值\n",
    "midvalue = np.loadtxt(open('',\"rb\"), delimiter=\",\", skiprows=0)\n",
    "file=open(r\"\",\"rb\")\n",
    "midmod = pickle.load(file)\n",
    "midvalue = midvalue\n",
    "nodenumx = [0]\n",
    "abnormalexg = np.zeros((1,args.node_num,1))\n",
    "trainid = np.zeros(1)\n",
    "nodenamelist = []\n",
    "abnormal = np.zeros((1,args.node_num,args.node_num))\n",
    "for ix in range(1,len(nodelistxx)+1): \n",
    "    abnormal1 = test_loader[6][nodenumxx[ix-1] : nodenumxx[ix]]\n",
    "    abnormalexg1 = np.array(test_loader[5]).reshape(-1,args.node_num,args.node_num)[nodenumxx[ix-1] : nodenumxx[ix]]\n",
    "    yy2 = test_loader[2][nodenumxx[ix-1] : nodenumxx[ix]]\n",
    "    ynum2 = test_loader[3][nodenumxx[ix-1] : nodenumxx[ix]]\n",
    "\n",
    "    ablike = np.array(torch.tensor(ablikely, device='cpu'))[nodenumxx[ix-1] : nodenumxx[ix]]\n",
    "    abnumlist = np.array(torch.tensor(num_ylist, device='cpu'))[nodenumxx[ix-1] : nodenumxx[ix]]\n",
    "    typelt = ynum2\n",
    "    num = []\n",
    "    for i in range(len(ablike)):\n",
    "        if ablike[i] < midvalue[int(abnumlist[i])]/1:\n",
    "            num.append(i)\n",
    "    nodenumx.append(len(num)+nodenumx[len(nodenumx)-1])\n",
    "    abnormalexg = np.concatenate((abnormalexg, abnormal1[num]), axis=0)\n",
    "    trainid = np.concatenate((trainid, ynum2[num]), axis=0)\n",
    "    nodenamelist.extend([yy2[iyt] for iyt in num])\n",
    "    abnormal = np.concatenate((abnormal, abnormalexg1[num].reshape(-1,args.node_num,args.node_num)), axis=0)\n",
    "\n",
    "abnormalexg = np.delete(abnormalexg, 0, axis=0)\n",
    "trainid = np.delete(trainid, 0, axis=0)   \n",
    "abnormal = np.delete(abnormal, 0, axis=0)\n",
    "edge = abnormal.reshape(-1,args.node_num,args.node_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b1cc9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from queue import LifoQueue\n",
    "def onlynode(abnormalnum, headnum, abormalfeatfun,tid):\n",
    "    weightindex = np.argwhere(edge[abnormalnum][headnum]==1)\n",
    "    weightnum = len(weightindex)\n",
    "    weighttree = np.repeat(abormalfeatfun.reshape((1,args.node_num)), repeats=1, axis=0)\n",
    "    j = 0\n",
    "    allexcess = 0\n",
    "    for i in range(weightnum):\n",
    "        excess = 0\n",
    "        excess += weighttree[j][weightindex[i][0]] - midmod[tid][weightindex[i][0]]\n",
    "        weighttree[j][weightindex[i][0]] = midmod[tid][weightindex[i][0]]\n",
    "        q=LifoQueue()\n",
    "        index = np.argwhere(edge[abnormalnum][weightindex[i][0]]==1)\n",
    "        for indexnum in range(len(index)):\n",
    "            q.put(index[indexnum][0])\n",
    "        while not q.empty():\n",
    "            qget = q.get()\n",
    "            weighttree[j][qget] = midmod[tid][qget]\n",
    "            index = np.argwhere(edge[abnormalnum][qget]==1)\n",
    "            for indexnum in range(len(index)):\n",
    "                q.put(index[indexnum][0])\n",
    "        allexcess+=excess\n",
    "        weighttree[j][headnum] = weighttree[j][headnum]-excess\n",
    "        index = np.argwhere(edge[abnormalnum][:,headnum]==1)\n",
    "        for indexnum in range(len(index)):\n",
    "            q.put(index[indexnum][0])\n",
    "        while not q.empty():\n",
    "            qget = q.get()\n",
    "            weighttree[j][qget] = weighttree[j][qget]-excess\n",
    "            index = np.argwhere(edge[abnormalnum][:,qget]==1)\n",
    "            for indexnum in range(len(index)):\n",
    "                q.put(index[indexnum][0])\n",
    "    return weighttree,allexcess\n",
    "def manynode(abnormalnum, headnum, abormalfeatfun,tid):\n",
    "    weightindex = np.argwhere(edge[abnormalnum][headnum]==1)\n",
    "    weightnum = len(weightindex)\n",
    "    weighttree = np.repeat(abormalfeatfun.reshape((1,args.node_num)), repeats=weightnum, axis=0)\n",
    "    from queue import LifoQueue\n",
    "    for i in range(weightnum):\n",
    "        excess = 0\n",
    "        new = np.delete(weightindex, i, axis=0)\n",
    "        for j in range(len(new)):\n",
    "            excess += weighttree[i][new[j][0]] - midmod[tid][new[j][0]]\n",
    "            weighttree[i][new[j][0]] = midmod[tid][new[j][0]]\n",
    "            q=LifoQueue()\n",
    "            index = np.argwhere(edge[abnormalnum][new[j][0]]==1)\n",
    "            for indexnum in range(len(index)):\n",
    "                q.put(index[indexnum][0])\n",
    "            while not q.empty():\n",
    "                qget = q.get()\n",
    "                weighttree[i][qget] = midmod[tid][qget]\n",
    "                index = np.argwhere(edge[abnormalnum][qget]==1)\n",
    "                for indexnum in range(len(index)):\n",
    "                    q.put(index[indexnum][0])\n",
    "        weighttree[i][headnum] = weighttree[i][headnum]-excess\n",
    "        index = np.argwhere(edge[abnormalnum][:,headnum]==1)\n",
    "        for indexnum in range(len(index)):\n",
    "            q.put(index[indexnum][0])\n",
    "        while not q.empty():\n",
    "            qget = q.get()\n",
    "            weighttree[i][qget] = weighttree[i][qget]-excess\n",
    "            index = np.argwhere(edge[abnormalnum][:,qget]==1)\n",
    "            for indexnum in range(len(index)):\n",
    "                q.put(index[indexnum][0]) \n",
    "    return weighttree,weightindex\n",
    "def modelpd(abnormalnum, weighttree, m,n):\n",
    "    edge_index_list = []\n",
    "    dfnp = abnormal[abnormalnum].reshape((-1,args.node_num,args.node_num))\n",
    "    dfnp = dfnp + np.identity(args.node_num)\n",
    "    for i in range(len(dfnp)):\n",
    "        size = np.sum(dfnp[i]==1)\n",
    "        midnp = np.zeros((2,size))\n",
    "        sign = 0\n",
    "        for j in range(args.node_num):\n",
    "            for t in range(args.node_num):\n",
    "                if dfnp[i][j][t] ==1:\n",
    "                    midnp[0][sign] =j\n",
    "                    midnp[1][sign] =t\n",
    "                    sign+=1\n",
    "        edge_index_list.append(midnp)\n",
    "\n",
    "    def normalization(matrix, mean, std):\n",
    "            n_mat = np.array(matrix, dtype=np.float32)\n",
    "            n_mat = np.where(n_mat<0.00001, 0, (n_mat - mean) / std)\n",
    "            return n_mat\n",
    "\n",
    "    graphfunp = weighttree.reshape((-1,args.node_num))\n",
    "    graphfunp = normalization(graphfunp, m, n)\n",
    "    graphfunp = graphfunp.reshape((-1,args.node_num,1))\n",
    "    like = []\n",
    "    with torch.no_grad():\n",
    "        for iteration in range(len(graphfunp)):\n",
    "            node_features = torch.tensor(graphfunp[iteration], dtype=torch.float32)\n",
    "            edge_index = torch.tensor(edge_index_list[0], dtype=torch.int64)\n",
    "            node_features = torch.squeeze(node_features, dim=0)\n",
    "            edge_index = torch.squeeze(edge_index)\n",
    "            edge_index = edge_index.to(device)\n",
    "            node_features = node_features.to(device)\n",
    "            loss,old_zb= vae.reconstruction_probability(node_features,edge_index)\n",
    "            like.append(old_zb)\n",
    "            like_tensor = torch.tensor(like)\n",
    "            like_cpu = like_tensor.cpu()\n",
    "            likely = np.array(like_cpu)\n",
    "    return likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce65918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abnodeall = []\n",
    "import queue\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "abnormalvalue = 1\n",
    "abnormalnodeall = []\n",
    "abnormallikeall = []\n",
    "for abnormalnum in tqdm(range(abnormalexg.shape[0])):\n",
    "    tid = int(trainid[abnormalnum])\n",
    "    abnormalnode = []\n",
    "    abnormallike = []\n",
    "    abormal0 = abnormalexg[abnormalnum].copy()\n",
    "    headnum = tid\n",
    "    weighttree,allexcess = onlynode(abnormalnum, headnum, abormal0,tid)\n",
    "    likely = modelpd(abnormalnum, weighttree, m,n)\n",
    "    if likely<midvalue[tid]/abnormalvalue:\n",
    "        abnormalnode.append(headnum)\n",
    "        ablike = []\n",
    "        ablike.append(headnum)\n",
    "        ablike.append(likely)\n",
    "        abnormallike.append(ablike)\n",
    "        abormal0[headnum] = midmod[tid][headnum]+allexcess\n",
    "    Q=queue.Queue()\n",
    "    index = np.argwhere(edge[abnormalnum][headnum]==1)\n",
    "    if len(index)==1:\n",
    "        headfeat = []\n",
    "        headfeat.append(index[0])\n",
    "        headfeat.append(abormal0)\n",
    "        headfeat.append(0)\n",
    "        Q.put(headfeat)\n",
    "    if len(index)>1:\n",
    "        headfeat = []\n",
    "        headfeat.append(tid)\n",
    "        headfeat.append(abormal0)\n",
    "        headfeat.append(1)\n",
    "        Q.put(headfeat)\n",
    "    while not Q.empty():\n",
    "        headfeat = Q.get()\n",
    "        if headfeat[2]==0:\n",
    "            weighttree,allexcess = onlynode(abnormalnum, headfeat[0][0], headfeat[1],tid)\n",
    "            likely = modelpd(abnormalnum, weighttree, m,n)\n",
    "            if likely<midvalue[tid]/abnormalvalue:\n",
    "                abnormalnode.append(headfeat[0][0])\n",
    "                ablike = []\n",
    "                ablike.append(headfeat[0][0])\n",
    "                ablike.append(likely)\n",
    "                abnormallike.append(ablike)\n",
    "                abormalcess = headfeat[1][headfeat[0][0]]-allexcess-midmod[tid][headfeat[0][0]]\n",
    "                headfeat[1][headfeat[0][0]] = allexcess+midmod[tid][headfeat[0][0]]\n",
    "                index = np.argwhere(edge[abnormalnum][:,headfeat[0][0]]==1)\n",
    "                q=LifoQueue()\n",
    "                for indexnum in range(len(index)):\n",
    "                    q.put(index[indexnum][0])\n",
    "                while not q.empty():\n",
    "                    qget = q.get()\n",
    "                    headfeat[1][qget] = headfeat[1][qget]-abormalcess\n",
    "                    index = np.argwhere(edge[abnormalnum][:,qget]==1)\n",
    "                    for indexnum in range(len(index)):\n",
    "                        q.put(index[indexnum][0])\n",
    "            index = np.argwhere(edge[abnormalnum][headfeat[0][0]]==1)\n",
    "            if len(index)==1:\n",
    "                headfeat1 = []\n",
    "                headfeat1.append(index[0])\n",
    "                headfeat1.append(headfeat[1])\n",
    "                headfeat1.append(0)\n",
    "                Q.put(headfeat1)\n",
    "            if len(index)>1:\n",
    "                headfeat1 = []\n",
    "                headfeat1.append(headfeat[0][0])\n",
    "                headfeat1.append(headfeat[1])\n",
    "                headfeat1.append(1)\n",
    "                Q.put(headfeat1)\n",
    "        if headfeat[2]==1:\n",
    "            weighttree,weightindex = manynode(abnormalnum, headfeat[0], headfeat[1],tid)\n",
    "            likely = modelpd(abnormalnum, weighttree, m,n)\n",
    "            abnormalindex = np.argwhere(likely<midvalue[tid]/abnormalvalue)\n",
    "            for indexnum in range(len(abnormalindex)):\n",
    "                headfeat1 = []\n",
    "                headfeat1.append(weightindex[abnormalindex[indexnum]][0])\n",
    "                headfeat1.append(weighttree[abnormalindex[indexnum]][0])\n",
    "                headfeat1.append(0)\n",
    "                Q.put(headfeat1)\n",
    "    def takeSecond(elem):\n",
    "        return elem[1]\n",
    "#     指定第二个元素排序\n",
    "    abnormallike.sort(key=takeSecond)\n",
    "    abnormalnodeall.append(abnormalnode)\n",
    "    abnormallikeall.append(abnormallike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47120640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abnodeall = []\n",
    "for ix in range(1,len(nodelistxx)+1):\n",
    "    abno = abnormalnodeall[nodenumx[ix-1]:nodenumx[ix]]\n",
    "    abnoli = nodenamelist[nodenumx[ix-1]:nodenumx[ix]]\n",
    "    abnoo = []\n",
    "    for ite in range(len(abno)):\n",
    "        for i in abno[ite]:\n",
    "            if i <args.trace_type:\n",
    "                abnoo.append(abnoli[ite][0])\n",
    "            else:\n",
    "                abnoo.append(abnoli[ite][i-(args.trace_type-1)])\n",
    "    abnodeall.append(abnoo)\n",
    "            \n",
    "import math\n",
    "def returnSum(myDict):   \n",
    "    sum = 0\n",
    "    for i in myDict: \n",
    "        sum = sum + myDict[i] \n",
    "    return sum\n",
    "\n",
    "yy4 = test_loader[2]\n",
    "\n",
    "top1 = 0\n",
    "top3 = 0\n",
    "top5 = 0\n",
    "nul = 0\n",
    "\n",
    "\n",
    "for ix in range(1,len(nodelistxx)+1):\n",
    "    abnode = abnodeall[ix-1]\n",
    "    nodedict = dict()\n",
    "    blyy = yy4[nodenumxx[ix-1]:nodenumxx[ix]]\n",
    "    for i in range(len(blyy)):\n",
    "        for noden in blyy[i]:\n",
    "            if \"->\" in noden:\n",
    "                nodesz = noden.split(\"->\")\n",
    "                nodesz[0] = \"net-\"+nodesz[0]\n",
    "                nodesz[1] = \"net-\"+nodesz[1]\n",
    "                if nodesz[0] in nodedict.keys():\n",
    "                    nodedict[nodesz[0]] = nodedict[nodesz[0]]+1\n",
    "                else:\n",
    "                    nodedict[nodesz[0]] = 1\n",
    "                if nodesz[1] in nodedict.keys():\n",
    "                    nodedict[nodesz[1]] = nodedict[nodesz[1]]+1\n",
    "                else:\n",
    "                    nodedict[nodesz[1]] = 1\n",
    "                continue\n",
    "            if noden in nodedict.keys():\n",
    "                nodedict[noden] = nodedict[noden]+1\n",
    "            else:\n",
    "                nodedict[noden] = 1\n",
    "    abnodedict = dict()\n",
    "    for noden in abnode:\n",
    "        if \"->\" in noden:\n",
    "            nodesz = noden.split(\"->\")\n",
    "            nodesz[0] = \"net-\"+nodesz[0]\n",
    "            nodesz[1] = \"net-\"+nodesz[1]\n",
    "            if nodesz[0] in abnodedict.keys():\n",
    "                abnodedict[nodesz[0]] = abnodedict[nodesz[0]]+1\n",
    "            else:\n",
    "                abnodedict[nodesz[0]] = 1\n",
    "            if nodesz[1] in abnodedict.keys():\n",
    "                abnodedict[nodesz[1]] = abnodedict[nodesz[1]]+1\n",
    "            else:\n",
    "                abnodedict[nodesz[1]] = 1\n",
    "            continue\n",
    "        if noden in abnodedict.keys():\n",
    "            abnodedict[noden] = abnodedict[noden]+1\n",
    "        else:\n",
    "            abnodedict[noden] = 1\n",
    "    abnodedict\n",
    "\n",
    "    scdict = dict()\n",
    "    for node in abnodedict.keys():\n",
    "        scdict[node] = abnodedict[node]/nodedict[node]\n",
    "        oef = abnodedict[node]\n",
    "        oep = nodedict[node] - abnodedict[node]\n",
    "        onf = returnSum(abnodedict) - abnodedict[node]\n",
    "        onp = returnSum(nodedict) - nodedict[node]\n",
    "        scdict[node] = oef/math.sqrt((oef+oep)*(oef+onf))\n",
    "    \n",
    "    score = dict()\n",
    "    for node in scdict.keys():\n",
    "        name = node.split(\"@\")[0]\n",
    "        if \"net\" in name:\n",
    "            name = name[4:]\n",
    "        name = str(name)\n",
    "        if name in score.keys():\n",
    "            score[name] = score[name] + scdict[node]\n",
    "        else:\n",
    "            score[name] = scdict[node]\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f\"------------------{nodelistxx[ix-1]}-----------------------------------------\")\n",
    "    retdict = sorted(score.items(), key=lambda x: -x[1])\n",
    "    namelistxl = str(nodelistxx[ix-1])\n",
    "    print(retdict)\n",
    "    scdict.clear()\n",
    "    nodedict.clear()\n",
    "    abnodedict.clear()\n",
    "    score.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df456b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3a3d48-a806-4c8b-b7c2-c72f2c368a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
